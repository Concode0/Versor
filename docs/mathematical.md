# Mathematical Foundations

A rigorous introduction to the mathematics behind Versor.

## 1. Clifford Algebra $Cl(p, q)$

A Clifford Algebra is generated by $n = p + q$ basis vectors {e_1, $\ldots$ e_n} satisfying:

$$e_i e_j + e_j e_i = 2 g_{ij}$$

where $g$ is the metric tensor with signature $(p, q)$:

$$e_i^2 = \begin{cases}
+1 & i \leq p \\
-1 & i > p
\end{cases}$$

For distinct basis vectors: $e_i e_j = -e_j e_i$ (they anti-commute).

### Basis Blades

Products of distinct basis vectors form **basis blades**. In $n$ dimensions there are $2^n$ basis blades, one for each subset of {e_1, $\ldots$ e_n}$:

| Grade | Count | Example ($n=3$) |
|-------|-------|----------|
| 0 (scalar) | $\binom{n}{0} = 1$ | $1$ |
| 1 (vector) | $\binom{n}{1} = n$ | $e_1, e_2, e_3$ |
| 2 (bivector) | $\binom{n}{2}$ | $e_{12}, e_{13}, e_{23}$ |
| 3 (trivector) | $\binom{n}{3}$ | $e_{123}$ |
| $k$ | $\binom{n}{k}$ | $k$-blade |

A **multivector** is a general element: $M = \sum_{I} m_I e_I$ where $I$ ranges over all subsets.

### Binary Indexing

Versor represents basis blades by their binary index. For $e_{i_1 \cdots i_k}$, the index is $2^{i_1-1} + \cdots + 2^{i_k-1}$:

- $1 \to 0$ (binary `000`)
- $e_1 \to 1$ (binary `001`)
- $e_2 \to 2$ (binary `010`)
- $e_{12} \to 3$ (binary `011`)
- $e_3 \to 4$ (binary `100`)

The **grade** of a blade equals the popcount (number of set bits) of its index.

## 2. The Geometric Product

The fundamental operation. For two multivectors $A$ and $B$:

$$(AB)_k = \sum_{i \oplus j = k} \sigma(i, j) \cdot A_i \cdot B_j$$

where $i \oplus j$ is XOR (the result blade index), and $\sigma(i, j)$ is the sign factor combining:

1. **Commutation sign**: Number of transpositions needed to reorder basis vectors. If basis vector $e_a$ in $A$ must pass $m$ basis vectors in $B$ that have lower index, the sign contribution is $(-1)^m$.

2. **Metric sign**: For each shared basis vector $e_k$ (where bit $k$ is set in both $i$ and $j$), if $k > p$ then $e_k^2 = -1$, contributing a factor of $-1$.

Versor precomputes these signs in the **Cayley table** (a $2^n \times 2^n$ lookup of indices and signs).

### Decomposition

The geometric product decomposes into:

$$ab = a \cdot b + a \wedge b$$

- **Inner product** $a \cdot b$: lowers grade (projection). Symmetric for vectors.
- **Outer product** $a \wedge b$: raises grade (area/volume). Anti-symmetric for vectors.

## 3. Rotors

A **rotor** is an even-grade multivector satisfying $R\tilde{R} = 1$ (unit versor condition). The **reversion** $\tilde{R}$ reverses the order of basis vectors in each blade:

$$\widetilde{e_{i_1 \cdots i_k}} = e_{i_k \cdots i_1} = (-1)^{k(k-1)/2} e_{i_1 \cdots i_k}$$

### Generating Rotors

A rotor for rotation by angle $\theta$ in the plane defined by unit bivector $\hat{B}$ is:

$$R = \exp(-\hat{B}\theta/2) = \cos(\theta/2) - \hat{B}\sin(\theta/2)$$

In Versor, the `RotorLayer` learns bivector weights $B$ (not necessarily unit) and computes $R = \exp(-B/2)$ using the scaling-and-squaring method with Taylor series of order 12.

### The Sandwich Product

A rotor transforms a multivector via the **sandwich product**:

$$x' = R x \tilde{R}$$

Properties:
- **Grade-preserving**: If $x$ is grade-$k$, so is $x'$
- **Isometric**: $|x'| = |x|$ (norm is preserved)
- **Composable**: $R_2(R_1 x \tilde{R}_1)\tilde{R}_2 = (R_2 R_1) x \widetilde{(R_2 R_1)}$
- **Smooth**: Continuous in the bivector parameters

### Parameter Count

A rotation in $n$ dimensions has $\binom{n}{2}$ degrees of freedom (one per bivector basis blade). Compare:
- Rotation matrix: $n^2$ parameters ($n(n-1)/2$ independent)
- Rotor: $\binom{n}{2}$ parameters (all independent, no redundancy)

## 4. Metric Signatures

### Euclidean $Cl(p, 0)$

All basis vectors square to $+1$. Rotors perform standard rotations.

- $Cl(3, 0)$: 8 blades, 3 bivectors. Standard 3D rotations (molecular geometry, point clouds).
- $Cl(6, 0)$: 64 blades, 15 bivectors. High-dimensional embedding rotations.

### Minkowski $Cl(p, q)$

Mixed signature. Basis vectors $e_1, \ldots, e_p$ square to $+1$; $e_{p+1}, \ldots, e_n$ square to $-1$.

- $Cl(1, 1)$: 4 blades, 1 bivector. The bivector $e_{12}$ generates **Lorentz boosts** (hyperbolic rotations) instead of circular rotations.
- $Cl(1, 3)$: Spacetime algebra. Rotors encode both spatial rotations and Lorentz boosts.

### Conformal $Cl(n+1, 1)$

Adds two extra dimensions to represent the point at infinity ($e_\infty$) and the origin ($e_o$). In this representation, translations, rotations, and dilations are all rotors.

- $Cl(4, 1)$: Conformal model of 3D Euclidean space.

## 5. Grade Projection

The grade-$k$ projection of a multivector extracts only components with exactly $k$ basis vectors:

$$\langle M \rangle_k = \sum_{\text{popcount}(I)=k} m_I e_I$$

Versor implements this by masking: for each index $I$, check if `popcount(I) == k`.

## 6. Scaling and Squaring for Exponentiation

Computing $\exp(A)$ directly via Taylor series can be numerically unstable for large $\|A\|$. Versor uses **scaling and squaring**:

1. Find $k$ such that $\|A\| / 2^k \leq 1$
2. Compute $E = \exp(A / 2^k)$ via Taylor series (order 12)
3. Square $k$ times: $\exp(A) = E^{2^k}$

This ensures the Taylor series converges quickly on a small argument, then recovers the full result through repeated squaring.

## 7. Multi-Rotor Superposition

A single rotor rotates in one plane. The `MultiRotorLayer` uses $K$ rotors in parallel:

$$x' = \sum_{k=1}^{K} w_k R_k x \tilde{R}_k$$

where $w_k$ are learned mixing weights and each $R_k = \exp(-B_k/2)$ rotates in a different plane. This is a **spectral decomposition** of the transformation: complex geometric operations are expressed as weighted sums of simple rotations.

Complexity: $O(K \cdot n^2)$ bivector parameters instead of $O(2^n)$ for a general multivector transformation.

## 8. Justification of the $O(K \cdot n^2)$ Approximation

The complexity reduction from $O(2^n)$ (for a full linear map on the multivector space) to $O(K \cdot n^2)$ (for $K$ rotors) is not merely an algorithmic optimization; it is rooted in deep geometric principles.

### Geometric Spectral Decomposition
Just as any signal can be decomposed into a sum of sinusoids (Fourier Transform), we posit that any **geometrically meaningful** transformation can be approximated by a weighted sum of simple rotations (rotors).
- A single rotor $R = \exp(-B/2)$ represents an atomic geometric operation.
- The weighted sum $\sum w_k R_k x \tilde{R}_k$ effectively constructs a "Fourier series" on the group of rotations (Spin group).
- By limiting $K$, we regularize the model to learn only the most significant geometric features, filtering out high-frequency noise that would require a dense $2^n \times 2^n$ matrix to represent.

### Cartan-Dieudonné Theorem
The **Cartan-Dieudonné Theorem** states that every orthogonal transformation in an $n$-dimensional space can be composed of at most $n$ reflections. Since a rotor is the product of two reflections, any rotation can be decomposed into a sequence of simple rotors.
- While the theorem applies to composition ($R = R_k \dots R_1$), our additive model $\sum w_k (R_k x \tilde{R}_k)$ can be seen as a linearized approximation or an ensemble of these fundamental paths.
- This theoretical guarantee suggests that the space of rotors is "rich enough" to cover the manifold of interest without needing the full linear group $GL(2^n)$.

### Geometric Sparsity
Real-world data is **geometrically sparse**.
- In a high-dimensional feature space (e.g., $n=32 \to 2^{32}$ dimensions), useful transformations rarely involve arbitrary mixings of all $2^{32}$ components.
- Meaningful operations tend to be local in "grade-space" (e.g., rotating vectors to vectors, bivectors to bivectors) and local in "basis-space" (involving only a few related planes).
- The $O(K \cdot n^2)$ parameterization enforces this sparsity. It forces the network to find the specific 2D planes (bivectors) where the action happens, rather than learning a dense matrix that statistically correlates everything with everything.

## 9. The Power of Non-Orthogonal Rotors

While the Cartan-Dieudonné theorem guarantees decomposition into a minimal set of orthogonal planes, Versor intentionally breaks this rigidity. We move from a minimal Basis to an overcomplete Frame

### Geometric Filtering
When multiple rotors operate in overlapping, non-orthogonal planes, they create complex **geometric interference patterns**. These patterns act as band-pass filters for manifold curvature, effectively pruning non-geometric noise. The network learns to "tune" these interferences to resonate only with the true geometric structure of the data, leading to a much cleaner and more representative latent space.

### Shared Backbone Effect
Non-orthogonal planes form an **overcomplete geometric frame**. This redundancy allows different feature channels to "share" the same latent geometric features, acting as a shared backbone. Much like multi-task learning, this forced sharing regularizes the model and allows it to generalize geometric truths across disparate parts of the input space.

### Redundancy for Robustness
By moving from rigid orthogonal bases to flexible, overcomplete non-orthogonal frames, Versor achieves significantly higher **resilience against data perturbations**. The redundancy in the frame means that if one part of the input is noisy or corrupted, the remaining overlapping rotors can still reconstruct the underlying geometry. Furthermore, this overparameterization (in terms of planes, not just raw weights) leads to smoother gradient flows during training, as the optimizer has multiple redundant geometric paths to reach the same transformation.

## 10. Lipschitz Continuity

A function $f: X \to Y$ is **Lipschitz continuous** with constant $K$ if:

$$d_Y(f(x_1), f(x_2)) \leq K d_X(x_1, x_2)$$

for all $x_1, x_2 \in X$. For neural networks, constraining $K$ (usually $K \approx 1$) is crucial for:

1.  **Robustness**: Limiting sensitivity to adversarial perturbations.
2.  **Stability**: Preventing gradient explosion/vanishing in deep networks.
3.  **Generative Modeling**: Ensuring the Kantorovich-Rubinstein duality holds for Wasserstein GANs (WGANs) and other optimal transport methods.

### The Isometric Advantage

In Versor, the core operation is the rotor sandwich product $x' = R x \tilde{R}$. Because rotors are unit versors ($R\tilde{R} = 1$), this operation is an **isometry**:

$$\|R x \tilde{R}\| = \|x\|$$

This means the rotor transformation naturally has a Lipschitz constant of **exactly 1**.

Unlike standard neural networks, which must use spectral normalization, weight clipping, or gradient penalties to *force* Lipschitz constraints (often approximately), Versor's RotorLayer satisfy this property **by construction** and GeometricGELU and CliffordLayerNorm only control Radial Direction's Scale.

**Note on Current Implementation:** While rotor operations are strictly isometric, the current `CliffordLinear` layer uses standard scalar weight matrices for channel mixing, which do not inherently guarantee 1-Lipschitz continuity. Our roadmap includes replacing these with compositions of irreducible rotors to achieve end-to-end geometric Lipschitz guarantees.

## 11. Riemannian Optimization on the Spin Group (Default)

Versor uses **Riemannian optimization by default**. Standard gradient descent treats parameter space as **flat Euclidean** space: $\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla \theta$. This ignores the **curved geometry** of the parameter manifold. For rotations, the natural manifold is the **Spin group** Spin$(n)$, which is a non-flat Riemannian manifold.

### The Spin Group and Lie Algebra

The **Spin group** Spin$(n)$ is the set of all unit rotors:

$$\text{Spin}(n) = \{ R \in \text{Cl}(n,0) \mid R\tilde{R} = 1 \}$$

This is a **curved manifold**, not a flat vector space. The associated **Lie algebra** is:

$$\mathfrak{so}(n) = \{ B \in \text{Cl}(n,0) \mid \text{grade}(B) = 2 \}$$

The Lie algebra $\mathfrak{so}(n)$ is the **tangent space** at the identity element of Spin$(n)$. Critically, $\mathfrak{so}(n)$ is **isomorphic to the bivector space** — the space of all grade-2 multivectors.

### Exponential Map

The **exponential map** connects the Lie algebra to the group:

$$\exp: \mathfrak{so}(n) \to \text{Spin}(n), \quad B \mapsto \exp(B)$$

In Versor, we compute $R = \exp(-B/2)$ using the **scaling-and-squaring** method (see Section 3). The exponential map is a **geodesic retraction**: it maps a tangent vector (bivector) to a point on the manifold (rotor) by following a geodesic (shortest path on the curved surface).

### Why Bivector Parameterization is Natural

Versor's layers parameterize rotors **indirectly** via bivectors:

$$B \in \mathfrak{so}(n) \quad \xrightarrow{\exp(-B/2)} \quad R \in \text{Spin}(n)$$

Since $B$ lives in the **Lie algebra** (a vector space), gradients $\nabla_B$ are automatically tangent vectors. This makes bivector parameterization the **natural choice** for Riemannian optimization.

### Riemannian Gradient Descent

A Riemannian gradient update on Spin$(n)$ follows:

$$R_{\text{new}} = R_{\text{old}} \cdot \exp(-\eta \nabla_B)$$

where $\nabla_B$ is the gradient in the Lie algebra. For bivector parameters, this becomes:

$$B_{\text{new}} = B_{\text{old}} - \eta \nabla_B$$

$$R_{\text{new}} = \exp(-B_{\text{new}}/2) = \exp(-(B_{\text{old}} - \eta \nabla_B)/2)$$

For small learning rates $\eta$, this approximates:

$$R_{\text{new}} \approx \exp(-B_{\text{old}}/2) \cdot \exp(\eta \nabla_B / 2) = R_{\text{old}} \cdot \exp(\eta \nabla_B / 2)$$

So **Euclidean updates in bivector space + exponential map in forward pass = Riemannian updates on Spin**$(n)$.

### Numerical Stability: Bivector Norm Clipping

Large bivector norms cause numerical issues in $\exp(-B/2)$:

- Scaling-and-squaring requires $k \sim \log_2(\|B\|)$ squarings
- Very large $k$ leads to compounding numerical errors, NaN, or overflow

To prevent this, Versor's Riemannian optimizers **clip bivector norms** after each update:

$$B \leftarrow B \cdot \min\left(1, \frac{\|B\|_{\max}}{\|B\|}\right)$$

where $\|B\|_{\max}$ (default: 10.0) is a user-specified maximum norm. This ensures $\|B\| \leq \|B\|_{\max}$, keeping the exponential map numerically stable.

Geometrically, this limits how far a single gradient step can move on the manifold, preventing "teleportation" across the curved surface.

### Implementation: ExponentialSGD and RiemannianAdam

Versor implements two Riemannian optimizers:

**ExponentialSGD**: SGD with momentum in the Lie algebra
```python
from optimizers.riemannian import ExponentialSGD

optimizer = ExponentialSGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    algebra=algebra,
    max_bivector_norm=10.0
)
```

**RiemannianAdam**: Adam with momentum accumulation in the Lie algebra
```python
from optimizers.riemannian import RiemannianAdam

optimizer = RiemannianAdam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    algebra=algebra,
    max_bivector_norm=10.0
)
```

Both optimizers:
1. Compute gradients $\nabla_B$ in the bivector (Lie algebra) space
2. Update bivector parameters: $B \leftarrow B - \eta \nabla_B$ (with momentum/adaptive scaling)
3. Clip bivector norms for numerical stability
4. Forward pass applies exponential map: $R = \exp(-B/2)$

This completes the Riemannian update on Spin$(n)$.

### Why Momentum Lives in the Lie Algebra

For momentum-based methods (SGD with momentum, Adam), the momentum buffers naturally live in the **Lie algebra** $\mathfrak{so}(n)$, not on the manifold Spin$(n)$. This is because:

1. The Lie algebra is a **vector space** — we can add and scale bivectors
2. Momentum is a linear combination of past gradients: $m_t = \beta m_{t-1} + (1-\beta) \nabla_B$
3. The manifold Spin$(n)$ is non-linear — adding rotors doesn't make sense

Full Riemannian Adam (with **parallel transport**) would transport momentum vectors along geodesics on the manifold. However, for bivector parameterization, this is unnecessary: momentum already lives in the tangent space where gradients are computed, so no transport is needed.

### Comparison to Euclidean Optimization

| Aspect | Euclidean (AdamW) | Riemannian (RiemannianAdam, Default) |
|--------|-------------------|--------------------------------------|
| **Status in Versor** | Ablation experiments only | **Default optimizer** |
| Parameter Space | Flat $\mathbb{R}^n$ | Curved Spin$(n)$ manifold |
| Update Rule | $\theta \leftarrow \theta - \eta \nabla\theta$ | $B \leftarrow B - \eta \nabla_B$, $R = \exp(-B/2)$ |
| Geometry | Ignores curvature | Respects geodesics |
| Stability | Clipping, normalization | Bivector norm clipping |
| Learning Rate | Typically 0.01 | Typically 0.001 |
| Theoretical | Approximate (incorrect for rotors) | Exact (first-order) |

Riemannian optimization is the **theoretically correct** approach for rotor parameters and is now the default in Versor.

### When to Use Riemannian Optimization

**Riemannian Optimizers (Default, Recommended)**:
- ✅ Training rotor-heavy architectures (RotorLayer, MultiRotorLayer, RotorGadget)
- ✅ Large rotations expected (bivector norms $\gg 0.1$)
- ✅ Theoretical correctness (true geometric optimization on Spin(n))
- ✅ Numerical stability with bivector norm clipping
- ✅ **All Versor tasks use this by default**

**Euclidean Optimizers (AdamW) - For Ablation Only**:
- ⚠️ Baseline comparison / ablation studies
- ⚠️ Reproducing legacy results
- ⚠️ Model has predominantly scalar parameters (non-rotor layers)
- ⚠️ Note: Treats Spin(n) as flat space, theoretically incorrect for rotor parameters

### References

1. **Absil, P.-A., Mahony, R., & Sepulchre, R.** (2008). *Optimization Algorithms on Matrix Manifolds*. Princeton University Press.
2. **Boumal, N.** (2023). *An Introduction to Optimization on Smooth Manifolds*. Cambridge University Press.
3. **Sola, J., Deray, J., & Atchuthan, D.** (2018). "A Micro Lie Theory for State Estimation in Robotics." arXiv:1812.01537

## 12. Hermitian Metrics for Mixed-Signature Algebras

In mixed-signature algebras $Cl(p,q)$ with $q > 0$, the standard trace form $\langle \tilde{A}B \rangle_0$ can be negative, breaking gradient-based optimization. The **Hermitian inner product** resolves this by using the Clifford conjugation (bar involution):

$$\langle A, B \rangle_H = \langle \bar{A} B \rangle_0 = \sum_I s_I \cdot a_I \cdot b_I$$

where $s_I = \text{conj\_sign}_I \cdot \text{metric\_sign}_I$ is a precomputed sign per basis element combining the Clifford conjugation sign $(-1)^k(-1)^{k(k-1)/2}$ with the metric sign $(-1)^{k(k-1)/2} \prod_{j \in I} g_{jj}$.

For **Euclidean** algebras $Cl(p,0)$, all signs are $+1$ and this reduces to the simple coefficient inner product $\sum a_I b_I$. For mixed signatures like $Cl(2,1)$ (spacetime) or $Cl(4,1)$ (conformal), the signs encode the algebraic structure properly.

The **Hermitian norm** $\|A\|_H = \sqrt{|\langle A, A \rangle_H|}$ and **Hermitian grade spectrum** $\{|\langle A_k, A_k \rangle_H|\}_{k=0}^n$ provide stable, algebraically meaningful measurements for optimization in any signature. The `HermitianGradeRegularization` loss uses this spectrum to guide grade energy distribution toward physically meaningful targets.
